{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbfdc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n",
      "âœ… Base path: c:\\NATIONAL AMR DATA ANALYSIS FILES\n",
      "âœ… Data path: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\n",
      "âœ… Reference data path: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\Database Resources\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options for better visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set up file paths with validation\n",
    "try:\n",
    "    BASE_PATH = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "    if not BASE_PATH.exists():\n",
    "        BASE_PATH = Path(r'c:\\NATIONAL AMR DATA ANALYSIS FILES')\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / 'data'\n",
    "    RAW_DATA_PATH = DATA_PATH / 'raw'\n",
    "    PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "    REFERENCE_DATA_PATH = DATA_PATH / 'Database Resources'\n",
    "    \n",
    "    # Validate all paths exist\n",
    "    for path_name, path_obj in [\n",
    "        ('Base', BASE_PATH),\n",
    "        ('Data', DATA_PATH),\n",
    "        ('Raw Data', RAW_DATA_PATH),\n",
    "        ('Reference Data', REFERENCE_DATA_PATH)\n",
    "    ]:\n",
    "        if not path_obj.exists():\n",
    "            print(f\"âš ï¸  Warning: {path_name} path does not exist: {path_obj}\")\n",
    "    \n",
    "    # Create processed data directory if it doesn't exist\n",
    "    PROCESSED_DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"âœ… Libraries imported successfully\")\n",
    "    print(f\"âœ… Base path: {BASE_PATH}\")\n",
    "    print(f\"âœ… Data path: {DATA_PATH}\")\n",
    "    print(f\"âœ… Reference data path: {REFERENCE_DATA_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error setting up paths: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5bc28",
   "metadata": {},
   "source": [
    "### WHO GLASS Quality Standards Configuration\n",
    "\n",
    "Define WHO GLASS specific quality standards and validation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2012cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configuring WHO GLASS Standards...\n",
      "âœ… WHO GLASS configuration completed\n",
      "ðŸ“‹ Essential fields configured: 8\n",
      "ðŸŽ¯ Quality thresholds set: 4\n",
      "ðŸ‘¶ Age categories defined: 4\n",
      "ðŸ’Š AWARE categories: 4\n",
      "ðŸ§ª AST columns available: 34\n",
      "ðŸ“Š Additional data fields: 3\n",
      "ðŸ¥ Department types: 2\n",
      "ðŸ” Total columns in dataset: 45\n"
     ]
    }
   ],
   "source": [
    "# WHO GLASS Essential Fields and Standards Configuration\n",
    "print(\"ðŸ”§ Configuring WHO GLASS Standards...\")\n",
    "\n",
    "# WHO GLASS Essential Fields (as per WHO GLASS Manual v2.1)\n",
    "GLASS_ESSENTIAL_FIELDS_ORIGINAL = [\n",
    "    'PATIENT_ID',    # Patient identifier\n",
    "    'ORGANISM',      # Organism identification\n",
    "    'SPEC_DATE',     # Specimen collection date\n",
    "    'COUNTRY_A',     # Country code\n",
    "    'INSTITUT',      # Healthcare institution\n",
    "    'DEPARTMENT',    # Hospital department\n",
    "    'AGE',           # Patient age\n",
    "    'SEX'            # Patient sex/gender\n",
    "]\n",
    "\n",
    "# Actual field names in our dataset (AMR_DATA_FINAL.csv)\n",
    "GLASS_ESSENTIAL_FIELDS_MAPPED = [\n",
    "    'PATIENT_ID',    # Patient identifier (unchanged)\n",
    "    'ORGANISM',      # Organism identification (unchanged)\n",
    "    'SPEC_DATE',     # Specimen collection date (unchanged)\n",
    "    'COUNTRY_A',     # Country code (unchanged)\n",
    "    'INSTITUT',      # Healthcare institution (unchanged)\n",
    "    'DEPARTMENT',    # Hospital department (unchanged)\n",
    "    'AGE',           # Patient age (unchanged)\n",
    "    'SEX'            # Patient sex/gender (unchanged)\n",
    "]\n",
    "\n",
    "# Column mapping dictionary (minimal mapping needed since fields match WHO GLASS standards)\n",
    "COLUMN_MAPPING = {\n",
    "    # Most fields are already in WHO GLASS format, minimal mapping required\n",
    "    'INSTITUT': 'INSTITUTION',      # Standardize institution name\n",
    "    'COUNTRY_A': 'COUNTRY',         # Standardize country field name\n",
    "    'ORGANISM': 'ORGANISM_CODE',   # Make organism source explicit\n",
    "}\n",
    "\n",
    "# Note: The following fields are already appropriately named and need no mapping:\n",
    "# - PATIENT_ID, SPEC_DATE, DEPARTMENT, AGE, SEX, REGION, ORG_TYPE\n",
    "\n",
    "# Additional data fields available in the dataset\n",
    "ADDITIONAL_DATA_FIELDS = [\n",
    "    'ROW_IDX',       # Row identifier\n",
    "    'REGION',        # Geographic region\n",
    "    'ORG_TYPE'       # Organism type classification\n",
    "]\n",
    "\n",
    "# AST (Antimicrobial Susceptibility Testing) columns in the dataset\n",
    "AST_COLUMNS_RAW = [\n",
    "    'AMC_ND20', 'AMK_ND30', 'AMP_ND10', 'AMX_ND30', 'AZM_ND15', 'CAZ_ND30',\n",
    "    'CHL_ND30', 'CIP_ND5', 'CLI_ND2', 'CLO_ND5', 'CRO_ND30', 'CTX_ND30',\n",
    "    'CXM_ND30', 'ERY_ND15', 'ETP_ND10', 'FEP_ND30', 'FLC_ND', 'FOX_ND30',\n",
    "    'GEN_ND10', 'LEX_ND30', 'LIN_ND4', 'LNZ_ND30', 'LVX_ND5', 'MEM_ND10',\n",
    "    'MNO_ND30', 'OXA_ND1', 'PEN_ND10', 'PNV_ND10', 'RIF_ND5', 'SXT_ND1_2',\n",
    "    'TCY_ND30', 'TGC_ND15', 'TZP_ND100', 'VAN_ND30'\n",
    "]\n",
    "\n",
    "# WHO GLASS Quality Thresholds\n",
    "GLASS_QUALITY_THRESHOLDS = {\n",
    "    'minimum_completeness': 80,    # Minimum completeness for essential fields\n",
    "    'temporal_coverage_months': 12, # Minimum months of data collection\n",
    "    'minimum_isolates': 100,       # Minimum isolates for meaningful analysis\n",
    "    'ast_completeness': 70         # Minimum AST completeness\n",
    "}\n",
    "\n",
    "# WHO Age Categories (as per WHO GLASS)\n",
    "GLASS_AGE_CATEGORIES = {\n",
    "    'Neonates': '0-27 days',\n",
    "    'Children': '28 days - 17 years',\n",
    "    'Adults': '18+ years',\n",
    "    'Unknown': 'Missing/Invalid age'\n",
    "}\n",
    "\n",
    "# WHO GLASS Specimen Types (common types) - Note: Dataset uses department types instead\n",
    "GLASS_SPECIMEN_TYPES = {\n",
    "    'BLOOD': 'Blood culture',\n",
    "    'URINE': 'Urine culture', \n",
    "    'WOUND': 'Wound/soft tissue',\n",
    "    'RESP': 'Respiratory specimen',\n",
    "    'CSF': 'Cerebrospinal fluid',\n",
    "    'OTHER': 'Other specimen types'\n",
    "}\n",
    "\n",
    "# Department types actually present in the dataset\n",
    "DEPARTMENT_TYPES = {\n",
    "    'Inp': 'Inpatient',\n",
    "    'Out': 'Outpatient'\n",
    "}\n",
    "\n",
    "# WHO AWARE Categories for antimicrobials\n",
    "AWARE_CATEGORIES = ['Access', 'Watch', 'Reserve', 'Not Listed']\n",
    "\n",
    "print(\"âœ… WHO GLASS configuration completed\")\n",
    "print(f\"ðŸ“‹ Essential fields configured: {len(GLASS_ESSENTIAL_FIELDS_ORIGINAL)}\")\n",
    "print(f\"ðŸŽ¯ Quality thresholds set: {len(GLASS_QUALITY_THRESHOLDS)}\")\n",
    "print(f\"ðŸ‘¶ Age categories defined: {len(GLASS_AGE_CATEGORIES)}\")\n",
    "print(f\"ðŸ’Š AWARE categories: {len(AWARE_CATEGORIES)}\")\n",
    "print(f\"ðŸ§ª AST columns available: {len(AST_COLUMNS_RAW)}\")\n",
    "print(f\"ðŸ“Š Additional data fields: {len(ADDITIONAL_DATA_FIELDS)}\")\n",
    "print(f\"ðŸ¥ Department types: {len(DEPARTMENT_TYPES)}\")\n",
    "print(f\"ðŸ” Total columns in dataset: {len(GLASS_ESSENTIAL_FIELDS_ORIGINAL) + len(AST_COLUMNS_RAW) + len(ADDITIONAL_DATA_FIELDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5862fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“‚ LOADING RAW AMR DATA\n",
      "======================================================================\n",
      "ðŸ“Š Loading data from: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\raw\\AMR_DATA_FINAL.csv\n",
      "âœ… Data loaded successfully!\n",
      "ðŸ“Š Dataset shape: (36173, 45)\n",
      "ðŸ“‹ Columns: ['ROW_IDX', 'COUNTRY_A', 'PATIENT_ID', 'SEX', 'AGE', 'INSTITUT', 'REGION', 'DEPARTMENT', 'SPEC_DATE', 'ORGANISM', 'ORG_TYPE', 'AMC_ND20', 'AMK_ND30', 'AMP_ND10', 'AMX_ND30', 'AZM_ND15', 'CAZ_ND30', 'CHL_ND30', 'CIP_ND5', 'CLI_ND2', 'CLO_ND5', 'CRO_ND30', 'CTX_ND30', 'CXM_ND30', 'ERY_ND15', 'ETP_ND10', 'FEP_ND30', 'FLC_ND', 'FOX_ND30', 'GEN_ND10', 'LEX_ND30', 'LIN_ND4', 'LNZ_ND30', 'LVX_ND5', 'MEM_ND10', 'MNO_ND30', 'OXA_ND1', 'PEN_ND10', 'PNV_ND10', 'RIF_ND5', 'SXT_ND1_2', 'TCY_ND30', 'TGC_ND15', 'TZP_ND100', 'VAN_ND30']\n",
      "\n",
      "======================================================================\n",
      "ðŸ”„ WHO GLASS COLUMN MAPPING & STANDARDIZATION\n",
      "======================================================================\n",
      "\n",
      "1. Column Mapping Analysis\n",
      "----------------------------------------\n",
      "Current columns requiring standardization:\n",
      "  â€¢ INSTITUT â†’ INSTITUTION\n",
      "  â€¢ COUNTRY_A â†’ COUNTRY\n",
      "  â€¢ ORGANISM â†’ ORGANISM_CODE\n",
      "\n",
      "ðŸ“Š Total columns requiring mapping: 3\n",
      "\n",
      "2. Applying Column Mapping\n",
      "----------------------------------------\n",
      "  âœ“ INSTITUT â†’ INSTITUTION\n",
      "  âœ“ COUNTRY_A â†’ COUNTRY\n",
      "  âœ“ ORGANISM â†’ ORGANISM_CODE\n",
      "\n",
      "âœ… Column mapping completed!\n",
      "ðŸ“Š Successfully mapped: 3/3 columns\n",
      "\n",
      "3. Updated Essential Fields Verification\n",
      "----------------------------------------\n",
      "WHO GLASS Essential Fields (after mapping):\n",
      "  1. PATIENT_ID - âœ… Available\n",
      "  2. ORGANISM_CODE (was: ORGANISM) - âœ… Available\n",
      "  3. SPEC_DATE - âœ… Available\n",
      "  4. COUNTRY (was: COUNTRY_A) - âœ… Available\n",
      "  5. INSTITUTION (was: INSTITUT) - âœ… Available\n",
      "  6. DEPARTMENT - âœ… Available\n",
      "  7. AGE - âœ… Available\n",
      "  8. SEX - âœ… Available\n",
      "\n",
      "ðŸ“Š Essential fields status: 8/8 available\n",
      "ðŸŽ¯ All WHO GLASS essential fields are present!\n",
      "\n",
      "4. Final Dataset Summary\n",
      "----------------------------------------\n",
      "ðŸ“Š Final dataset shape: (36173, 45)\n",
      "ðŸ“‹ Final column count: 45\n",
      "ðŸ”„ Columns renamed: 3\n",
      "âœ… Data cleaning step 1 (Column Mapping) completed!\n",
      "\n",
      "ðŸ“ Column mapping summary saved to processing log\n",
      "\n",
      "======================================================================\n",
      "ðŸ” COLUMN MAPPING VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "1. Before/After Column Comparison\n",
      "----------------------------------------\n",
      "Key column transformations:\n",
      "  INSTITUT        â†’ INSTITUTION\n",
      "  COUNTRY_A       â†’ COUNTRY\n",
      "  ORGANISM        â†’ ORGANISM_CODE\n",
      "\n",
      "Original dataset columns (45):\n",
      "   1. ROW_IDX\n",
      "   2. COUNTRY_A ðŸ”„\n",
      "   3. PATIENT_ID\n",
      "   4. SEX\n",
      "   5. AGE\n",
      "   6. INSTITUT ðŸ”„\n",
      "   7. REGION\n",
      "   8. DEPARTMENT\n",
      "   9. SPEC_DATE\n",
      "  10. ORGANISM ðŸ”„\n",
      "  11. ORG_TYPE\n",
      "  12. AMC_ND20\n",
      "  13. AMK_ND30\n",
      "  14. AMP_ND10\n",
      "  15. AMX_ND30\n",
      "  16. AZM_ND15\n",
      "  17. CAZ_ND30\n",
      "  18. CHL_ND30\n",
      "  19. CIP_ND5\n",
      "  20. CLI_ND2\n",
      "  21. CLO_ND5\n",
      "  22. CRO_ND30\n",
      "  23. CTX_ND30\n",
      "  24. CXM_ND30\n",
      "  25. ERY_ND15\n",
      "  26. ETP_ND10\n",
      "  27. FEP_ND30\n",
      "  28. FLC_ND\n",
      "  29. FOX_ND30\n",
      "  30. GEN_ND10\n",
      "  31. LEX_ND30\n",
      "  32. LIN_ND4\n",
      "  33. LNZ_ND30\n",
      "  34. LVX_ND5\n",
      "  35. MEM_ND10\n",
      "  36. MNO_ND30\n",
      "  37. OXA_ND1\n",
      "  38. PEN_ND10\n",
      "  39. PNV_ND10\n",
      "  40. RIF_ND5\n",
      "  41. SXT_ND1_2\n",
      "  42. TCY_ND30\n",
      "  43. TGC_ND15\n",
      "  44. TZP_ND100\n",
      "  45. VAN_ND30\n",
      "\n",
      "Standardized dataset columns (45):\n",
      "   1. ROW_IDX\n",
      "   2. COUNTRY âœ¨\n",
      "   3. PATIENT_ID\n",
      "   4. SEX\n",
      "   5. AGE\n",
      "   6. INSTITUTION âœ¨\n",
      "   7. REGION\n",
      "   8. DEPARTMENT\n",
      "   9. SPEC_DATE\n",
      "  10. ORGANISM_CODE âœ¨\n",
      "  11. ORG_TYPE\n",
      "  12. AMC_ND20\n",
      "  13. AMK_ND30\n",
      "  14. AMP_ND10\n",
      "  15. AMX_ND30\n",
      "  16. AZM_ND15\n",
      "  17. CAZ_ND30\n",
      "  18. CHL_ND30\n",
      "  19. CIP_ND5\n",
      "  20. CLI_ND2\n",
      "  21. CLO_ND5\n",
      "  22. CRO_ND30\n",
      "  23. CTX_ND30\n",
      "  24. CXM_ND30\n",
      "  25. ERY_ND15\n",
      "  26. ETP_ND10\n",
      "  27. FEP_ND30\n",
      "  28. FLC_ND\n",
      "  29. FOX_ND30\n",
      "  30. GEN_ND10\n",
      "  31. LEX_ND30\n",
      "  32. LIN_ND4\n",
      "  33. LNZ_ND30\n",
      "  34. LVX_ND5\n",
      "  35. MEM_ND10\n",
      "  36. MNO_ND30\n",
      "  37. OXA_ND1\n",
      "  38. PEN_ND10\n",
      "  39. PNV_ND10\n",
      "  40. RIF_ND5\n",
      "  41. SXT_ND1_2\n",
      "  42. TCY_ND30\n",
      "  43. TGC_ND15\n",
      "  44. TZP_ND100\n",
      "  45. VAN_ND30\n",
      "\n",
      "2. WHO GLASS Essential Fields Status\n",
      "----------------------------------------\n",
      "  PATIENT_ID           - 100.0% complete âœ…\n",
      "  ORGANISM_CODE        - 100.0% complete âœ…\n",
      "  SPEC_DATE            - 100.0% complete âœ…\n",
      "  COUNTRY              - 100.0% complete âœ…\n",
      "  INSTITUTION          - 100.0% complete âœ…\n",
      "  DEPARTMENT           - 100.0% complete âœ…\n",
      "  AGE                  -  89.6% complete âœ…\n",
      "  SEX                  -  96.0% complete âœ…\n",
      "\n",
      "Overall WHO GLASS Compliance: 8/8 fields meet minimum thresholds\n",
      "\n",
      "3. Data Sample After Mapping\n",
      "----------------------------------------\n",
      "Sample of standardized data (first 3 rows, essential fields only):\n",
      "\n",
      "Record 1:\n",
      "  PATIENT_ID          : _2917564954_\n",
      "  ORGANISM_CODE       : eco\n",
      "  SPEC_DATE           : 01-Jan-20\n",
      "  COUNTRY             : GHA\n",
      "  INSTITUTION         : N/A\n",
      "  DEPARTMENT          : Out\n",
      "  AGE                 : 44.0\n",
      "  SEX                 : f\n",
      "\n",
      "Record 2:\n",
      "  PATIENT_ID          : 10978\n",
      "  ORGANISM_CODE       : ac-\n",
      "  SPEC_DATE           : 01-Jan-22\n",
      "  COUNTRY             : GHA\n",
      "  INSTITUTION         : CCTH\n",
      "  DEPARTMENT          : Out\n",
      "  AGE                 : 1.0\n",
      "  SEX                 : f\n",
      "\n",
      "Record 3:\n",
      "  PATIENT_ID          : 12981\n",
      "  ORGANISM_CODE       : ac-\n",
      "  SPEC_DATE           : 01-Jan-22\n",
      "  COUNTRY             : GHA\n",
      "  INSTITUTION         : CCTH\n",
      "  DEPARTMENT          : Out\n",
      "  AGE                 : 0.0\n",
      "  SEX                 : m\n",
      "\n",
      "======================================================================\n",
      "âœ… COLUMN MAPPING VERIFICATION COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Data Loading and Initial Processing\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‚ LOADING RAW AMR DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the raw AMR data\n",
    "raw_data_file = RAW_DATA_PATH / 'AMR_DATA_FINAL.csv'\n",
    "\n",
    "try:\n",
    "    print(f\"ðŸ“Š Loading data from: {raw_data_file}\")\n",
    "    df_raw = pd.read_csv(raw_data_file)\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"ðŸ“Š Dataset shape: {df_raw.shape}\")\n",
    "    print(f\"ðŸ“‹ Columns: {df_raw.columns.tolist()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Error: Raw data file not found at {raw_data_file}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# WHO GLASS Column Mapping and Standardization\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”„ WHO GLASS COLUMN MAPPING & STANDARDIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_cleaned = df_raw.copy()\n",
    "\n",
    "print(\"\\n1. Column Mapping Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check which columns need mapping\n",
    "print(\"Current columns requiring standardization:\")\n",
    "columns_to_map = []\n",
    "for original_col, mapped_col in COLUMN_MAPPING.items():\n",
    "    if original_col in df_cleaned.columns:\n",
    "        columns_to_map.append((original_col, mapped_col))\n",
    "        print(f\"  â€¢ {original_col} â†’ {mapped_col}\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  {original_col} not found in dataset\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total columns requiring mapping: {len(columns_to_map)}\")\n",
    "\n",
    "print(\"\\n2. Applying Column Mapping\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Apply the column mapping\n",
    "columns_mapped = 0\n",
    "mapping_log = []\n",
    "\n",
    "for original_col, mapped_col in columns_to_map:\n",
    "    if original_col in df_cleaned.columns:\n",
    "        # Apply the mapping\n",
    "        df_cleaned = df_cleaned.rename(columns={original_col: mapped_col})\n",
    "        columns_mapped += 1\n",
    "        mapping_log.append(f\"âœ“ {original_col} â†’ {mapped_col}\")\n",
    "        print(f\"  âœ“ {original_col} â†’ {mapped_col}\")\n",
    "    else:\n",
    "        mapping_log.append(f\"âœ— {original_col} (not found)\")\n",
    "        print(f\"  âœ— {original_col} (not found)\")\n",
    "\n",
    "print(f\"\\nâœ… Column mapping completed!\")\n",
    "print(f\"ðŸ“Š Successfully mapped: {columns_mapped}/{len(COLUMN_MAPPING)} columns\")\n",
    "\n",
    "print(\"\\n3. Updated Essential Fields Verification\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Update the essential fields list to use mapped names\n",
    "GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED = []\n",
    "for field in GLASS_ESSENTIAL_FIELDS_ORIGINAL:\n",
    "    mapped_field = COLUMN_MAPPING.get(field, field)\n",
    "    GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED.append(mapped_field)\n",
    "\n",
    "print(\"WHO GLASS Essential Fields (after mapping):\")\n",
    "field_status = []\n",
    "for i, field in enumerate(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED):\n",
    "    if field in df_cleaned.columns:\n",
    "        status = \"âœ… Available\"\n",
    "        field_status.append(True)\n",
    "    else:\n",
    "        status = \"âŒ Missing\"\n",
    "        field_status.append(False)\n",
    "    \n",
    "    original_field = GLASS_ESSENTIAL_FIELDS_ORIGINAL[i]\n",
    "    if original_field != field:\n",
    "        print(f\"  {i+1}. {field} (was: {original_field}) - {status}\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. {field} - {status}\")\n",
    "\n",
    "available_fields = sum(field_status)\n",
    "print(f\"\\nðŸ“Š Essential fields status: {available_fields}/{len(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED)} available\")\n",
    "\n",
    "if available_fields == len(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED):\n",
    "    print(\"ðŸŽ¯ All WHO GLASS essential fields are present!\")\n",
    "else:\n",
    "    missing_fields = [field for field, status in zip(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED, field_status) if not status]\n",
    "    print(f\"âš ï¸  Missing essential fields: {missing_fields}\")\n",
    "\n",
    "print(\"\\n4. Final Dataset Summary\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"ðŸ“Š Final dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"ðŸ“‹ Final column count: {len(df_cleaned.columns)}\")\n",
    "print(f\"ðŸ”„ Columns renamed: {columns_mapped}\")\n",
    "print(f\"âœ… Data cleaning step 1 (Column Mapping) completed!\")\n",
    "\n",
    "# Save mapping log for documentation\n",
    "mapping_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'original_columns': len(df_raw.columns),\n",
    "    'final_columns': len(df_cleaned.columns),\n",
    "    'columns_mapped': columns_mapped,\n",
    "    'mapping_details': mapping_log,\n",
    "    'essential_fields_available': f\"{available_fields}/{len(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED)}\"\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“ Column mapping summary saved to processing log\")\n",
    "\n",
    "# =============================================================================\n",
    "# Column Mapping Verification and Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ” COLUMN MAPPING VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Before/After Column Comparison\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show original vs mapped column names\n",
    "print(\"Key column transformations:\")\n",
    "for original, mapped in COLUMN_MAPPING.items():\n",
    "    if original in df_raw.columns:\n",
    "        print(f\"  {original:15} â†’ {mapped}\")\n",
    "\n",
    "print(f\"\\nOriginal dataset columns ({len(df_raw.columns)}):\")\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    marker = \" ðŸ”„\" if col in COLUMN_MAPPING.keys() else \"\"\n",
    "    print(f\"  {i:2d}. {col}{marker}\")\n",
    "\n",
    "print(f\"\\nStandardized dataset columns ({len(df_cleaned.columns)}):\")\n",
    "for i, col in enumerate(df_cleaned.columns, 1):\n",
    "    marker = \" âœ¨\" if col in COLUMN_MAPPING.values() else \"\"\n",
    "    print(f\"  {i:2d}. {col}{marker}\")\n",
    "\n",
    "print(\"\\n2. WHO GLASS Essential Fields Status\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "essential_status = {}\n",
    "for field in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED:\n",
    "    if field in df_cleaned.columns:\n",
    "        completeness = (df_cleaned[field].notna().sum() / len(df_cleaned)) * 100\n",
    "        essential_status[field] = completeness\n",
    "        status = \"âœ…\" if completeness >= GLASS_QUALITY_THRESHOLDS['minimum_completeness'] else \"âš ï¸\"\n",
    "        print(f\"  {field:20} - {completeness:5.1f}% complete {status}\")\n",
    "    else:\n",
    "        essential_status[field] = 0.0\n",
    "        print(f\"  {field:20} - Missing âŒ\")\n",
    "\n",
    "overall_compliance = len([c for c in essential_status.values() if c >= GLASS_QUALITY_THRESHOLDS['minimum_completeness']])\n",
    "print(f\"\\nOverall WHO GLASS Compliance: {overall_compliance}/{len(essential_status)} fields meet minimum thresholds\")\n",
    "\n",
    "print(\"\\n3. Data Sample After Mapping\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show a sample of the mapped data\n",
    "print(\"Sample of standardized data (first 3 rows, essential fields only):\")\n",
    "sample_cols = [col for col in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED if col in df_cleaned.columns]\n",
    "sample_data = df_cleaned[sample_cols].head(3)\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows()):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for col in sample_cols:\n",
    "        value = row[col] if pd.notna(row[col]) else \"N/A\"\n",
    "        print(f\"  {col:20}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… COLUMN MAPPING VERIFICATION COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef315bfd",
   "metadata": {},
   "source": [
    "\n",
    "## WHO GLASS First Isolate Deduplication\n",
    "\n",
    "This section implements the WHO GLASS standard for first isolate selection per patient, which is critical for accurate antimicrobial resistance surveillance by ensuring each patient contributes only one isolate per organism per time period.\n",
    "\n",
    "### WHO GLASS Deduplication Rules:\n",
    "1. **Patient-based grouping**: Group records by patient identifier\n",
    "2. **Organism-specific**: Apply deduplication per organism type  \n",
    "3. **Temporal logic**: Select first isolate per patient per organism\n",
    "4. **Data integrity**: Preserve all AST results from the selected isolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f605578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_IDX</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INSTITUTION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>SPEC_DATE</th>\n",
       "      <th>ORGANISM_CODE</th>\n",
       "      <th>ORG_TYPE</th>\n",
       "      <th>AMC_ND20</th>\n",
       "      <th>AMK_ND30</th>\n",
       "      <th>AMP_ND10</th>\n",
       "      <th>AMX_ND30</th>\n",
       "      <th>AZM_ND15</th>\n",
       "      <th>CAZ_ND30</th>\n",
       "      <th>CHL_ND30</th>\n",
       "      <th>CIP_ND5</th>\n",
       "      <th>CLI_ND2</th>\n",
       "      <th>CLO_ND5</th>\n",
       "      <th>CRO_ND30</th>\n",
       "      <th>CTX_ND30</th>\n",
       "      <th>CXM_ND30</th>\n",
       "      <th>ERY_ND15</th>\n",
       "      <th>ETP_ND10</th>\n",
       "      <th>FEP_ND30</th>\n",
       "      <th>FLC_ND</th>\n",
       "      <th>FOX_ND30</th>\n",
       "      <th>GEN_ND10</th>\n",
       "      <th>LEX_ND30</th>\n",
       "      <th>LIN_ND4</th>\n",
       "      <th>LNZ_ND30</th>\n",
       "      <th>LVX_ND5</th>\n",
       "      <th>MEM_ND10</th>\n",
       "      <th>MNO_ND30</th>\n",
       "      <th>OXA_ND1</th>\n",
       "      <th>PEN_ND10</th>\n",
       "      <th>PNV_ND10</th>\n",
       "      <th>RIF_ND5</th>\n",
       "      <th>SXT_ND1_2</th>\n",
       "      <th>TCY_ND30</th>\n",
       "      <th>TGC_ND15</th>\n",
       "      <th>TZP_ND100</th>\n",
       "      <th>VAN_ND30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>GHA</td>\n",
       "      <td>_2917564954_</td>\n",
       "      <td>f</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-20</td>\n",
       "      <td>eco</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184</td>\n",
       "      <td>GHA</td>\n",
       "      <td>10978</td>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CCTH</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-22</td>\n",
       "      <td>ac-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>GHA</td>\n",
       "      <td>12981</td>\n",
       "      <td>m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCTH</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-22</td>\n",
       "      <td>ac-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186</td>\n",
       "      <td>GHA</td>\n",
       "      <td>CC160</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCTH</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>Inp</td>\n",
       "      <td>01-Jan-23</td>\n",
       "      <td>ac-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>247</td>\n",
       "      <td>GHA</td>\n",
       "      <td>_0123294111_</td>\n",
       "      <td>f</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CCTH</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>Inp</td>\n",
       "      <td>01-Jan-20</td>\n",
       "      <td>ci-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36168</th>\n",
       "      <td>31062</td>\n",
       "      <td>GHA</td>\n",
       "      <td>N536</td>\n",
       "      <td>m</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TTH</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-21</td>\n",
       "      <td>xxx</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36169</th>\n",
       "      <td>31063</td>\n",
       "      <td>GHA</td>\n",
       "      <td>N537</td>\n",
       "      <td>m</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TTH</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-21</td>\n",
       "      <td>xxx</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36170</th>\n",
       "      <td>31064</td>\n",
       "      <td>GHA</td>\n",
       "      <td>N538</td>\n",
       "      <td>f</td>\n",
       "      <td>20.0</td>\n",
       "      <td>TTH</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-21</td>\n",
       "      <td>xxx</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36171</th>\n",
       "      <td>31065</td>\n",
       "      <td>GHA</td>\n",
       "      <td>N539</td>\n",
       "      <td>m</td>\n",
       "      <td>33.0</td>\n",
       "      <td>TTH</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-21</td>\n",
       "      <td>xxx</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36172</th>\n",
       "      <td>31066</td>\n",
       "      <td>GHA</td>\n",
       "      <td>N540</td>\n",
       "      <td>m</td>\n",
       "      <td>18.0</td>\n",
       "      <td>TTH</td>\n",
       "      <td>Northern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>01-Jan-21</td>\n",
       "      <td>xxx</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36173 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROW_IDX COUNTRY    PATIENT_ID SEX   AGE INSTITUTION           REGION  \\\n",
       "0          501     GHA  _2917564954_   f  44.0         NaN              NaN   \n",
       "1          184     GHA         10978   f   1.0        CCTH   Central Region   \n",
       "2          185     GHA         12981   m   0.0        CCTH   Central Region   \n",
       "3          186     GHA         CC160   f   NaN        CCTH   Central Region   \n",
       "4          247     GHA  _0123294111_   f   2.0        CCTH   Central Region   \n",
       "...        ...     ...           ...  ..   ...         ...              ...   \n",
       "36168    31062     GHA          N536   m   1.0         TTH  Northern Region   \n",
       "36169    31063     GHA          N537   m   1.0         TTH  Northern Region   \n",
       "36170    31064     GHA          N538   f  20.0         TTH  Northern Region   \n",
       "36171    31065     GHA          N539   m  33.0         TTH  Northern Region   \n",
       "36172    31066     GHA          N540   m  18.0         TTH  Northern Region   \n",
       "\n",
       "      DEPARTMENT  SPEC_DATE ORGANISM_CODE ORG_TYPE AMC_ND20 AMK_ND30 AMP_ND10  \\\n",
       "0            Out  01-Jan-20           eco        -      NaN        S        R   \n",
       "1            Out  01-Jan-22           ac-        -      NaN      NaN      NaN   \n",
       "2            Out  01-Jan-22           ac-        -      NaN      NaN      NaN   \n",
       "3            Inp  01-Jan-23           ac-        -      NaN      NaN      NaN   \n",
       "4            Inp  01-Jan-20           ci-        -      NaN        R        R   \n",
       "...          ...        ...           ...      ...      ...      ...      ...   \n",
       "36168        Out  01-Jan-21           xxx        o      NaN      NaN      NaN   \n",
       "36169        Out  01-Jan-21           xxx        o      NaN      NaN      NaN   \n",
       "36170        Out  01-Jan-21           xxx        o      NaN      NaN      NaN   \n",
       "36171        Out  01-Jan-21           xxx        o      NaN      NaN      NaN   \n",
       "36172        Out  01-Jan-21           xxx        o      NaN      NaN      NaN   \n",
       "\n",
       "      AMX_ND30 AZM_ND15 CAZ_ND30 CHL_ND30 CIP_ND5 CLI_ND2 CLO_ND5 CRO_ND30  \\\n",
       "0          NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "1          NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "2          NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "3          NaN      NaN      NaN      NaN       R     NaN     NaN        R   \n",
       "4          NaN      NaN      NaN      NaN       R     NaN     NaN      NaN   \n",
       "...        ...      ...      ...      ...     ...     ...     ...      ...   \n",
       "36168      NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "36169      NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "36170      NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "36171      NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "36172      NaN      NaN      NaN      NaN     NaN     NaN     NaN      NaN   \n",
       "\n",
       "      CTX_ND30 CXM_ND30 ERY_ND15 ETP_ND10 FEP_ND30 FLC_ND FOX_ND30 GEN_ND10  \\\n",
       "0            S        R      NaN      NaN      NaN    NaN      NaN        R   \n",
       "1          NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "2          NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "3          NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "4            S        R      NaN      NaN      NaN    NaN      NaN        R   \n",
       "...        ...      ...      ...      ...      ...    ...      ...      ...   \n",
       "36168      NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "36169      NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "36170      NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "36171      NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "36172      NaN      NaN      NaN      NaN      NaN    NaN      NaN      NaN   \n",
       "\n",
       "      LEX_ND30 LIN_ND4 LNZ_ND30 LVX_ND5 MEM_ND10 MNO_ND30 OXA_ND1 PEN_ND10  \\\n",
       "0          NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "1          NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "2          NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "3          NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "4          NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "...        ...     ...      ...     ...      ...      ...     ...      ...   \n",
       "36168      NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "36169      NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "36170      NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "36171      NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "36172      NaN     NaN      NaN     NaN      NaN      NaN     NaN      NaN   \n",
       "\n",
       "      PNV_ND10 RIF_ND5 SXT_ND1_2 TCY_ND30 TGC_ND15 TZP_ND100 VAN_ND30  \n",
       "0          NaN     NaN         S        S      NaN       NaN      NaN  \n",
       "1          NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "2          NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "3          NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "4          NaN     NaN         S        S      NaN       NaN      NaN  \n",
       "...        ...     ...       ...      ...      ...       ...      ...  \n",
       "36168      NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "36169      NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "36170      NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "36171      NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "36172      NaN     NaN       NaN      NaN      NaN       NaN      NaN  \n",
       "\n",
       "[36173 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "420b7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”„ WHO GLASS FIRST ISOLATE DEDUPLICATION\n",
      "======================================================================\n",
      "\n",
      "1. Pre-Deduplication Analysis\n",
      "----------------------------------------\n",
      "ðŸ“Š Records before deduplication: 36,173\n",
      "ðŸ‘¥ Unique patients: 30,081\n",
      "ðŸ¦  Unique organisms: 76\n",
      "ðŸ“ˆ Average isolates per patient: 1.2\n",
      "ðŸ“Š Max isolates per patient: 3\n",
      "ðŸ‘¥ Patients with multiple isolates: 5,744 (19.1%)\n",
      "\n",
      "2. WHO GLASS Deduplication Parameters\n",
      "----------------------------------------\n",
      "ðŸ”‘ Deduplication keys: ['PATIENT_ID', 'ORGANISM_CODE']\n",
      "ðŸ“… Sort order: ['PATIENT_ID', 'ORGANISM_CODE', 'SPEC_DATE']\n",
      "âš¡ Strategy: Keep first isolate per patient per organism\n",
      "âš ï¸  PATIENT_ID: 6 missing (0.0%)\n",
      "âœ… ORGANISM_CODE: No missing values\n",
      "âœ… SPEC_DATE: No missing values\n",
      "\n",
      "3. Applying First Isolate Selection\n",
      "----------------------------------------\n",
      "ðŸ“… Converting specimen dates...\n",
      "ðŸ”„ Sorting data by patient, organism, and date...\n",
      "ðŸŽ¯ Selecting first isolate per patient per organism...\n",
      "âœ… Deduplication completed!\n",
      "ðŸ“Š Records before: 36,173\n",
      "ðŸ“Š Records after: 32,688\n",
      "ðŸ“‰ Records removed: 3,485\n",
      "ðŸ“Š Reduction rate: 9.6%\n",
      "\n",
      "4. Post-Deduplication Validation\n",
      "----------------------------------------\n",
      "ðŸ” Validating deduplication results...\n",
      "âœ… Validation passed: No duplicate patient-organism combinations remain\n",
      "ðŸ” Checking data integrity...\n",
      "Essential field completeness comparison:\n",
      "  PATIENT_ID          : 100.0% â†’ 100.0% ðŸ“ˆ\n",
      "  ORGANISM_CODE       : 100.0% â†’ 100.0% âž¡ï¸\n",
      "  SPEC_DATE           : 100.0% â†’ 100.0% âž¡ï¸\n",
      "  COUNTRY             : 100.0% â†’ 100.0% âž¡ï¸\n",
      "  INSTITUTION         : 100.0% â†’ 100.0% ðŸ“‰\n",
      "  DEPARTMENT          : 100.0% â†’ 100.0% âž¡ï¸\n",
      "  AGE                 :  89.6% â†’  89.3% ðŸ“‰\n",
      "  SEX                 :  96.0% â†’  96.1% ðŸ“ˆ\n",
      "\n",
      "5. Deduplication Summary\n",
      "----------------------------------------\n",
      "ðŸ‘¥ Unique patients: 30,081 â†’ 30,081\n",
      "ðŸ¦  Unique organisms: 76 â†’ 76\n",
      "ðŸ“Š Duplicate analysis:\n",
      "  â€¢ 3,053 patient-organism pairs had 2 isolates each (removed 3,053 duplicates)\n",
      "  â€¢ 213 patient-organism pairs had 3 isolates each (removed 426 duplicates)\n",
      "ðŸ“… Date range preserved: 1096 â†’ 1096 days\n",
      "\n",
      "======================================================================\n",
      "âœ… WHO GLASS FIRST ISOLATE DEDUPLICATION COMPLETED\n",
      "======================================================================\n",
      "ðŸ“Š Updated working dataset: 32,688 records\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š DEDUPLICATION RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. Sample of Deduplicated Data\n",
      "----------------------------------------\n",
      "Sample records from first 5 patients:\n",
      "\n",
      "Patient 1 (ID: 0101):\n",
      "  â€¢ Records: 1\n",
      "  â€¢ Organisms: xxx\n",
      "  â€¢ Date: 2023-01-01\n",
      "\n",
      "Patient 2 (ID: 0105/23):\n",
      "  â€¢ Records: 1\n",
      "  â€¢ Organisms: xxx\n",
      "  â€¢ Date: 2023-01-01\n",
      "\n",
      "Patient 3 (ID: 0106):\n",
      "  â€¢ Records: 1\n",
      "  â€¢ Organisms: xxx\n",
      "  â€¢ Date: 2023-01-01\n",
      "\n",
      "Patient 4 (ID: 0107):\n",
      "  â€¢ Records: 1\n",
      "  â€¢ Organisms: xxx\n",
      "  â€¢ Date: 2023-01-01\n",
      "\n",
      "Patient 5 (ID: 0112/23):\n",
      "  â€¢ Records: 1\n",
      "  â€¢ Organisms: xxx\n",
      "  â€¢ Date: 2023-01-01\n",
      "\n",
      "2. Organism Distribution Analysis\n",
      "----------------------------------------\n",
      "Top 10 organisms (before â†’ after deduplication):\n",
      "   1. xxx             28,382 â†’ 24,924 (12.2% reduction)\n",
      "   2. scn             1,596 â†’ 1,588 ( 0.5% reduction)\n",
      "   3. sau             1,562 â†’ 1,558 ( 0.3% reduction)\n",
      "   4. sep             959 â†’ 958 ( 0.1% reduction)\n",
      "   5. sta             752 â†’ 750 ( 0.3% reduction)\n",
      "   6. kpn             559 â†’ 558 ( 0.2% reduction)\n",
      "   7. eco             403 â†’ 403 ( 0.0% reduction)\n",
      "   8. en-             355 â†’ 353 ( 0.6% reduction)\n",
      "   9. pae             236 â†’ 235 ( 0.4% reduction)\n",
      "  10. ci-             203 â†’ 200 ( 1.5% reduction)\n",
      "\n",
      "3. Patient-Level Impact Assessment\n",
      "----------------------------------------\n",
      "Patient impact by original isolate count:\n",
      "  â€¢ 24,337 patients with 1 isolates â†’ No change\n",
      "  â€¢ 5,402 patients with 2 isolates â†’ Reduced to 1-2 isolates\n",
      "  â€¢ 342 patients with 3 isolates â†’ Reduced to 1-3 isolates\n",
      "\n",
      "4. Quality Metrics Summary\n",
      "----------------------------------------\n",
      "ðŸ“Š Final Quality Metrics:\n",
      "  â€¢ Total Records: 32,688\n",
      "  â€¢ Unique Patients: 30,081\n",
      "  â€¢ Unique Organisms: 76\n",
      "  â€¢ Date Range Days: 1,096\n",
      "  â€¢ Avg Isolates Per Patient: 1.09\n",
      "  â€¢ Deduplication Rate: 9.63\n",
      "\n",
      "5. WHO GLASS Compliance Check\n",
      "----------------------------------------\n",
      "WHO GLASS Compliance Status:\n",
      "  â€¢ First Isolate Rule: âœ… PASS\n",
      "  â€¢ Patient Id Present: âœ… PASS\n",
      "  â€¢ Organism Code Present: âœ… PASS\n",
      "  â€¢ Specimen Date Present: âœ… PASS\n",
      "  â€¢ Minimum Records: âœ… PASS\n",
      "\n",
      "ðŸŽ¯ Overall WHO GLASS Compliance: 100.0%\n",
      "âœ… Dataset meets WHO GLASS quality standards!\n",
      "\n",
      "======================================================================\n",
      "âœ… DEDUPLICATION ANALYSIS COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# WHO GLASS First Isolate Deduplication\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”„ WHO GLASS FIRST ISOLATE DEDUPLICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Pre-Deduplication Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check current data status\n",
    "print(f\"ðŸ“Š Records before deduplication: {len(df_cleaned):,}\")\n",
    "print(f\"ðŸ‘¥ Unique patients: {df_cleaned['PATIENT_ID'].nunique():,}\")\n",
    "print(f\"ðŸ¦  Unique organisms: {df_cleaned['ORGANISM_CODE'].nunique():,}\")\n",
    "\n",
    "# Analyze patient-isolate distribution\n",
    "patient_isolate_counts = df_cleaned.groupby('PATIENT_ID').size()\n",
    "print(f\"ðŸ“ˆ Average isolates per patient: {patient_isolate_counts.mean():.1f}\")\n",
    "print(f\"ðŸ“Š Max isolates per patient: {patient_isolate_counts.max()}\")\n",
    "\n",
    "# Show patients with multiple isolates\n",
    "multiple_isolates = patient_isolate_counts[patient_isolate_counts > 1]\n",
    "print(f\"ðŸ‘¥ Patients with multiple isolates: {len(multiple_isolates):,} ({len(multiple_isolates)/len(patient_isolate_counts)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n2. WHO GLASS Deduplication Parameters\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define deduplication columns (patient + organism + optional time period)\n",
    "dedup_columns = ['PATIENT_ID', 'ORGANISM_CODE']\n",
    "sort_columns = ['PATIENT_ID', 'ORGANISM_CODE', 'SPEC_DATE']\n",
    "\n",
    "print(f\"ðŸ”‘ Deduplication keys: {dedup_columns}\")\n",
    "print(f\"ðŸ“… Sort order: {sort_columns}\")\n",
    "print(f\"âš¡ Strategy: Keep first isolate per patient per organism\")\n",
    "\n",
    "# Check for missing values in critical fields\n",
    "missing_data = {}\n",
    "for col in dedup_columns + ['SPEC_DATE']:\n",
    "    missing_count = df_cleaned[col].isna().sum()\n",
    "    missing_pct = (missing_count / len(df_cleaned)) * 100\n",
    "    missing_data[col] = {'count': missing_count, 'percentage': missing_pct}\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(f\"âš ï¸  {col}: {missing_count:,} missing ({missing_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"âœ… {col}: No missing values\")\n",
    "\n",
    "print(\"\\n3. Applying First Isolate Selection\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Convert SPEC_DATE to datetime for proper sorting\n",
    "print(\"ðŸ“… Converting specimen dates...\")\n",
    "df_cleaned['SPEC_DATE'] = pd.to_datetime(df_cleaned['SPEC_DATE'], errors='coerce')\n",
    "\n",
    "# Check for date conversion issues\n",
    "invalid_dates = df_cleaned['SPEC_DATE'].isna().sum()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"âš ï¸  Warning: {invalid_dates} records have invalid dates\")\n",
    "\n",
    "# Sort data by patient, organism, and date (earliest first)\n",
    "print(\"ðŸ”„ Sorting data by patient, organism, and date...\")\n",
    "df_sorted = df_cleaned.sort_values(sort_columns, na_position='last')\n",
    "\n",
    "# Apply first isolate selection using groupby and first()\n",
    "print(\"ðŸŽ¯ Selecting first isolate per patient per organism...\")\n",
    "df_first_isolate = df_sorted.groupby(dedup_columns, as_index=False).first()\n",
    "\n",
    "# Calculate deduplication metrics\n",
    "records_before = len(df_cleaned)\n",
    "records_after = len(df_first_isolate)\n",
    "records_removed = records_before - records_after\n",
    "reduction_rate = (records_removed / records_before) * 100\n",
    "\n",
    "print(f\"âœ… Deduplication completed!\")\n",
    "print(f\"ðŸ“Š Records before: {records_before:,}\")\n",
    "print(f\"ðŸ“Š Records after: {records_after:,}\")\n",
    "print(f\"ðŸ“‰ Records removed: {records_removed:,}\")\n",
    "print(f\"ðŸ“Š Reduction rate: {reduction_rate:.1f}%\")\n",
    "\n",
    "print(\"\\n4. Post-Deduplication Validation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Validate deduplication results\n",
    "print(\"ðŸ” Validating deduplication results...\")\n",
    "\n",
    "# Check for any remaining duplicates\n",
    "remaining_duplicates = df_first_isolate.groupby(dedup_columns).size()\n",
    "max_duplicates = remaining_duplicates.max()\n",
    "\n",
    "if max_duplicates == 1:\n",
    "    print(\"âœ… Validation passed: No duplicate patient-organism combinations remain\")\n",
    "else:\n",
    "    print(f\"âŒ Validation failed: Found {max_duplicates} max duplicates\")\n",
    "\n",
    "# Check data integrity\n",
    "print(\"ðŸ” Checking data integrity...\")\n",
    "essential_fields_check = []\n",
    "for field in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED:\n",
    "    if field in df_first_isolate.columns:\n",
    "        before_completeness = (df_cleaned[field].notna().sum() / len(df_cleaned)) * 100\n",
    "        after_completeness = (df_first_isolate[field].notna().sum() / len(df_first_isolate)) * 100\n",
    "        essential_fields_check.append({\n",
    "            'field': field,\n",
    "            'before': before_completeness,\n",
    "            'after': after_completeness,\n",
    "            'change': after_completeness - before_completeness\n",
    "        })\n",
    "\n",
    "print(\"Essential field completeness comparison:\")\n",
    "for check in essential_fields_check:\n",
    "    change_indicator = \"ðŸ“ˆ\" if check['change'] > 0 else \"ðŸ“‰\" if check['change'] < 0 else \"âž¡ï¸\"\n",
    "    print(f\"  {check['field']:20}: {check['before']:5.1f}% â†’ {check['after']:5.1f}% {change_indicator}\")\n",
    "\n",
    "print(\"\\n5. Deduplication Summary\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create summary statistics\n",
    "unique_patients_before = df_cleaned['PATIENT_ID'].nunique()\n",
    "unique_patients_after = df_first_isolate['PATIENT_ID'].nunique()\n",
    "unique_organisms_before = df_cleaned['ORGANISM_CODE'].nunique()\n",
    "unique_organisms_after = df_first_isolate['ORGANISM_CODE'].nunique()\n",
    "\n",
    "print(f\"ðŸ‘¥ Unique patients: {unique_patients_before:,} â†’ {unique_patients_after:,}\")\n",
    "print(f\"ðŸ¦  Unique organisms: {unique_organisms_before:,} â†’ {unique_organisms_after:,}\")\n",
    "\n",
    "# Analyze the removed records\n",
    "if records_removed > 0:\n",
    "    print(f\"ðŸ“Š Duplicate analysis:\")\n",
    "    duplicate_analysis = df_cleaned.groupby(dedup_columns).size().value_counts().sort_index()\n",
    "    for isolate_count, patient_count in duplicate_analysis.items():\n",
    "        if isolate_count > 1:\n",
    "            total_duplicates = patient_count * (isolate_count - 1)\n",
    "            print(f\"  â€¢ {patient_count:,} patient-organism pairs had {isolate_count} isolates each (removed {total_duplicates:,} duplicates)\")\n",
    "\n",
    "# Date range analysis\n",
    "date_range_before = df_cleaned['SPEC_DATE'].max() - df_cleaned['SPEC_DATE'].min()\n",
    "date_range_after = df_first_isolate['SPEC_DATE'].max() - df_first_isolate['SPEC_DATE'].min()\n",
    "print(f\"ðŸ“… Date range preserved: {date_range_before.days} â†’ {date_range_after.days} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… WHO GLASS FIRST ISOLATE DEDUPLICATION COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Update the main dataframe\n",
    "df_cleaned = df_first_isolate.copy()\n",
    "print(f\"ðŸ“Š Updated working dataset: {len(df_cleaned):,} records\")\n",
    "\n",
    "# =============================================================================\n",
    "# Deduplication Results Analysis and Verification\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š DEDUPLICATION RESULTS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Sample of Deduplicated Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show sample of final deduplicated data\n",
    "sample_patients = df_cleaned['PATIENT_ID'].unique()[:5]\n",
    "print(\"Sample records from first 5 patients:\")\n",
    "\n",
    "for i, patient_id in enumerate(sample_patients, 1):\n",
    "    patient_data = df_cleaned[df_cleaned['PATIENT_ID'] == patient_id]\n",
    "    print(f\"\\nPatient {i} (ID: {patient_id}):\")\n",
    "    print(f\"  â€¢ Records: {len(patient_data)}\")\n",
    "    print(f\"  â€¢ Organisms: {', '.join(patient_data['ORGANISM_CODE'].unique())}\")\n",
    "    print(f\"  â€¢ Date: {patient_data['SPEC_DATE'].dt.date.iloc[0] if not patient_data['SPEC_DATE'].isna().iloc[0] else 'N/A'}\")\n",
    "\n",
    "print(\"\\n2. Organism Distribution Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze organism distribution before and after\n",
    "organism_counts_before = df_raw.groupby('ORGANISM').size().sort_values(ascending=False)\n",
    "organism_counts_after = df_cleaned.groupby('ORGANISM_CODE').size().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 organisms (before â†’ after deduplication):\")\n",
    "top_organisms = organism_counts_after.head(10)\n",
    "\n",
    "for i, (organism, count_after) in enumerate(top_organisms.items(), 1):\n",
    "    count_before = organism_counts_before.get(organism, 0)\n",
    "    reduction = count_before - count_after if count_before > 0 else 0\n",
    "    reduction_pct = (reduction / count_before * 100) if count_before > 0 else 0\n",
    "    \n",
    "    print(f\"  {i:2d}. {organism:15} {count_before:,} â†’ {count_after:,} ({reduction_pct:4.1f}% reduction)\")\n",
    "\n",
    "print(\"\\n3. Patient-Level Impact Assessment\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze impact on different patient categories\n",
    "patient_impact = []\n",
    "\n",
    "# Group patients by their original isolate count\n",
    "original_patient_counts = df_raw.groupby('PATIENT_ID').size()\n",
    "final_patient_counts = df_cleaned.groupby('PATIENT_ID').size()\n",
    "\n",
    "for isolate_count in sorted(original_patient_counts.unique()):\n",
    "    patients_with_count = (original_patient_counts == isolate_count).sum()\n",
    "    if isolate_count == 1:\n",
    "        retained = patients_with_count  # All single-isolate patients retained\n",
    "        impact = \"No change\"\n",
    "    else:\n",
    "        retained = (final_patient_counts <= isolate_count).sum()\n",
    "        impact = f\"Reduced to 1-{isolate_count} isolates\"\n",
    "    \n",
    "    patient_impact.append({\n",
    "        'original_isolates': isolate_count,\n",
    "        'patient_count': patients_with_count,\n",
    "        'impact': impact\n",
    "    })\n",
    "\n",
    "print(\"Patient impact by original isolate count:\")\n",
    "for impact in patient_impact[:10]:  # Show first 10 categories\n",
    "    print(f\"  â€¢ {impact['patient_count']:,} patients with {impact['original_isolates']} isolates â†’ {impact['impact']}\")\n",
    "\n",
    "print(\"\\n4. Quality Metrics Summary\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate final quality metrics\n",
    "quality_metrics = {\n",
    "    'total_records': len(df_cleaned),\n",
    "    'unique_patients': df_cleaned['PATIENT_ID'].nunique(),\n",
    "    'unique_organisms': df_cleaned['ORGANISM_CODE'].nunique(),\n",
    "    'date_range_days': (df_cleaned['SPEC_DATE'].max() - df_cleaned['SPEC_DATE'].min()).days,\n",
    "    'avg_isolates_per_patient': len(df_cleaned) / df_cleaned['PATIENT_ID'].nunique(),\n",
    "    'deduplication_rate': reduction_rate\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Final Quality Metrics:\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  â€¢ {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {metric.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "print(\"\\n5. WHO GLASS Compliance Check\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Final WHO GLASS compliance validation\n",
    "compliance_checks = {\n",
    "    'first_isolate_rule': max_duplicates == 1,\n",
    "    'patient_id_present': df_cleaned['PATIENT_ID'].notna().all(),\n",
    "    'organism_code_present': df_cleaned['ORGANISM_CODE'].notna().all(),\n",
    "    'specimen_date_present': df_cleaned['SPEC_DATE'].notna().sum() / len(df_cleaned) > 0.8,\n",
    "    'minimum_records': len(df_cleaned) >= GLASS_QUALITY_THRESHOLDS['minimum_isolates']\n",
    "}\n",
    "\n",
    "print(\"WHO GLASS Compliance Status:\")\n",
    "for check, passed in compliance_checks.items():\n",
    "    status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n",
    "    print(f\"  â€¢ {check.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "overall_compliance = sum(compliance_checks.values()) / len(compliance_checks) * 100\n",
    "print(f\"\\nðŸŽ¯ Overall WHO GLASS Compliance: {overall_compliance:.1f}%\")\n",
    "\n",
    "if overall_compliance >= 90:\n",
    "    print(\"âœ… Dataset meets WHO GLASS quality standards!\")\n",
    "elif overall_compliance >= 70:\n",
    "    print(\"âš ï¸  Dataset partially meets WHO GLASS standards - review required\")\n",
    "else:\n",
    "    print(\"âŒ Dataset does not meet WHO GLASS standards - significant issues detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DEDUPLICATION ANALYSIS COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581db846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th>ORGANISM_CODE</th>\n",
       "      <th>ROW_IDX</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INSTITUTION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>SPEC_DATE</th>\n",
       "      <th>ORG_TYPE</th>\n",
       "      <th>AMC_ND20</th>\n",
       "      <th>AMK_ND30</th>\n",
       "      <th>AMP_ND10</th>\n",
       "      <th>AMX_ND30</th>\n",
       "      <th>AZM_ND15</th>\n",
       "      <th>CAZ_ND30</th>\n",
       "      <th>CHL_ND30</th>\n",
       "      <th>CIP_ND5</th>\n",
       "      <th>CLI_ND2</th>\n",
       "      <th>CLO_ND5</th>\n",
       "      <th>CRO_ND30</th>\n",
       "      <th>CTX_ND30</th>\n",
       "      <th>CXM_ND30</th>\n",
       "      <th>ERY_ND15</th>\n",
       "      <th>ETP_ND10</th>\n",
       "      <th>FEP_ND30</th>\n",
       "      <th>FLC_ND</th>\n",
       "      <th>FOX_ND30</th>\n",
       "      <th>GEN_ND10</th>\n",
       "      <th>LEX_ND30</th>\n",
       "      <th>LIN_ND4</th>\n",
       "      <th>LNZ_ND30</th>\n",
       "      <th>LVX_ND5</th>\n",
       "      <th>MEM_ND10</th>\n",
       "      <th>MNO_ND30</th>\n",
       "      <th>OXA_ND1</th>\n",
       "      <th>PEN_ND10</th>\n",
       "      <th>PNV_ND10</th>\n",
       "      <th>RIF_ND5</th>\n",
       "      <th>SXT_ND1_2</th>\n",
       "      <th>TCY_ND30</th>\n",
       "      <th>TGC_ND15</th>\n",
       "      <th>TZP_ND100</th>\n",
       "      <th>VAN_ND30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101</td>\n",
       "      <td>xxx</td>\n",
       "      <td>18097</td>\n",
       "      <td>GHA</td>\n",
       "      <td>f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ERH</td>\n",
       "      <td>Eastern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0105/23</td>\n",
       "      <td>xxx</td>\n",
       "      <td>18098</td>\n",
       "      <td>GHA</td>\n",
       "      <td>m</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ERH</td>\n",
       "      <td>Eastern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0106</td>\n",
       "      <td>xxx</td>\n",
       "      <td>18099</td>\n",
       "      <td>GHA</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERH</td>\n",
       "      <td>Eastern Region</td>\n",
       "      <td>Inp</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0107</td>\n",
       "      <td>xxx</td>\n",
       "      <td>18100</td>\n",
       "      <td>GHA</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERH</td>\n",
       "      <td>Eastern Region</td>\n",
       "      <td>Inp</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0112/23</td>\n",
       "      <td>xxx</td>\n",
       "      <td>18101</td>\n",
       "      <td>GHA</td>\n",
       "      <td>f</td>\n",
       "      <td>29.0</td>\n",
       "      <td>ERH</td>\n",
       "      <td>Eastern Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32683</th>\n",
       "      <td>t95</td>\n",
       "      <td>xxx</td>\n",
       "      <td>31159</td>\n",
       "      <td>GHA</td>\n",
       "      <td>f</td>\n",
       "      <td>7.0</td>\n",
       "      <td>HTH</td>\n",
       "      <td>Volta Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32684</th>\n",
       "      <td>t96</td>\n",
       "      <td>xxx</td>\n",
       "      <td>31160</td>\n",
       "      <td>GHA</td>\n",
       "      <td>m</td>\n",
       "      <td>66.0</td>\n",
       "      <td>HTH</td>\n",
       "      <td>Volta Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32685</th>\n",
       "      <td>t97</td>\n",
       "      <td>xxx</td>\n",
       "      <td>31161</td>\n",
       "      <td>GHA</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HTH</td>\n",
       "      <td>Volta Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32686</th>\n",
       "      <td>t98</td>\n",
       "      <td>xxx</td>\n",
       "      <td>31162</td>\n",
       "      <td>GHA</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HTH</td>\n",
       "      <td>Volta Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>o</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32687</th>\n",
       "      <td>t99</td>\n",
       "      <td>scn</td>\n",
       "      <td>4245</td>\n",
       "      <td>GHA</td>\n",
       "      <td>m</td>\n",
       "      <td>36.0</td>\n",
       "      <td>HTH</td>\n",
       "      <td>Volta Region</td>\n",
       "      <td>Out</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>+</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32688 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PATIENT_ID ORGANISM_CODE  ROW_IDX COUNTRY SEX   AGE INSTITUTION  \\\n",
       "0           0101           xxx    18097     GHA   f   1.0         ERH   \n",
       "1        0105/23           xxx    18098     GHA   m  41.0         ERH   \n",
       "2           0106           xxx    18099     GHA   m   NaN         ERH   \n",
       "3           0107           xxx    18100     GHA   m   NaN         ERH   \n",
       "4        0112/23           xxx    18101     GHA   f  29.0         ERH   \n",
       "...          ...           ...      ...     ...  ..   ...         ...   \n",
       "32683        t95           xxx    31159     GHA   f   7.0         HTH   \n",
       "32684        t96           xxx    31160     GHA   m  66.0         HTH   \n",
       "32685        t97           xxx    31161     GHA   f   0.0         HTH   \n",
       "32686        t98           xxx    31162     GHA   f   0.0         HTH   \n",
       "32687        t99           scn     4245     GHA   m  36.0         HTH   \n",
       "\n",
       "               REGION DEPARTMENT  SPEC_DATE ORG_TYPE AMC_ND20 AMK_ND30  \\\n",
       "0      Eastern Region        Out 2023-01-01        o     None     None   \n",
       "1      Eastern Region        Out 2023-01-01        o     None     None   \n",
       "2      Eastern Region        Inp 2023-01-01        o     None     None   \n",
       "3      Eastern Region        Inp 2023-01-01        o     None     None   \n",
       "4      Eastern Region        Out 2023-01-01        o     None     None   \n",
       "...               ...        ...        ...      ...      ...      ...   \n",
       "32683    Volta Region        Out 2021-01-01        o     None     None   \n",
       "32684    Volta Region        Out 2021-01-01        o     None     None   \n",
       "32685    Volta Region        Out 2021-01-01        o     None     None   \n",
       "32686    Volta Region        Out 2021-01-01        o     None     None   \n",
       "32687    Volta Region        Out 2021-01-01        +     None     None   \n",
       "\n",
       "      AMP_ND10 AMX_ND30 AZM_ND15 CAZ_ND30 CHL_ND30 CIP_ND5 CLI_ND2 CLO_ND5  \\\n",
       "0         None     None     None     None     None    None    None    None   \n",
       "1         None     None     None     None     None    None    None    None   \n",
       "2         None     None     None     None     None    None    None    None   \n",
       "3         None     None     None     None     None    None    None    None   \n",
       "4         None     None     None     None     None    None    None    None   \n",
       "...        ...      ...      ...      ...      ...     ...     ...     ...   \n",
       "32683     None     None     None     None     None    None    None    None   \n",
       "32684     None     None     None     None     None    None    None    None   \n",
       "32685     None     None     None     None     None    None    None    None   \n",
       "32686     None     None     None     None     None    None    None    None   \n",
       "32687        R     None     None     None     None       S    None    None   \n",
       "\n",
       "      CRO_ND30 CTX_ND30 CXM_ND30 ERY_ND15 ETP_ND10 FEP_ND30 FLC_ND FOX_ND30  \\\n",
       "0         None     None     None     None     None     None   None     None   \n",
       "1         None     None     None     None     None     None   None     None   \n",
       "2         None     None     None     None     None     None   None     None   \n",
       "3         None     None     None     None     None     None   None     None   \n",
       "4         None     None     None     None     None     None   None     None   \n",
       "...        ...      ...      ...      ...      ...      ...    ...      ...   \n",
       "32683     None     None     None     None     None     None   None     None   \n",
       "32684     None     None     None     None     None     None   None     None   \n",
       "32685     None     None     None     None     None     None   None     None   \n",
       "32686     None     None     None     None     None     None   None     None   \n",
       "32687     None     None     None     None     None     None   None     None   \n",
       "\n",
       "      GEN_ND10 LEX_ND30 LIN_ND4 LNZ_ND30 LVX_ND5 MEM_ND10 MNO_ND30 OXA_ND1  \\\n",
       "0         None     None    None     None    None     None     None    None   \n",
       "1         None     None    None     None    None     None     None    None   \n",
       "2         None     None    None     None    None     None     None    None   \n",
       "3         None     None    None     None    None     None     None    None   \n",
       "4         None     None    None     None    None     None     None    None   \n",
       "...        ...      ...     ...      ...     ...      ...      ...     ...   \n",
       "32683     None     None    None     None    None     None     None    None   \n",
       "32684     None     None    None     None    None     None     None    None   \n",
       "32685     None     None    None     None    None     None     None    None   \n",
       "32686     None     None    None     None    None     None     None    None   \n",
       "32687        S     None    None     None    None     None     None    None   \n",
       "\n",
       "      PEN_ND10 PNV_ND10 RIF_ND5 SXT_ND1_2 TCY_ND30 TGC_ND15 TZP_ND100 VAN_ND30  \n",
       "0         None     None    None      None     None     None      None     None  \n",
       "1         None     None    None      None     None     None      None     None  \n",
       "2         None     None    None      None     None     None      None     None  \n",
       "3         None     None    None      None     None     None      None     None  \n",
       "4         None     None    None      None     None     None      None     None  \n",
       "...        ...      ...     ...       ...      ...      ...       ...      ...  \n",
       "32683     None     None    None      None     None     None      None     None  \n",
       "32684     None     None    None      None     None     None      None     None  \n",
       "32685     None     None    None      None     None     None      None     None  \n",
       "32686     None     None    None      None     None     None      None     None  \n",
       "32687     None     None    None      None     None     None      None     None  \n",
       "\n",
       "[32688 rows x 45 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e409d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset columns:\n",
      "['PATIENT_ID', 'ORGANISM_CODE', 'ROW_IDX', 'COUNTRY', 'SEX', 'AGE', 'INSTITUTION', 'REGION', 'DEPARTMENT', 'SPEC_DATE', 'ORG_TYPE', 'AMC_ND20', 'AMK_ND30', 'AMP_ND10', 'AMX_ND30', 'AZM_ND15', 'CAZ_ND30', 'CHL_ND30', 'CIP_ND5', 'CLI_ND2', 'CLO_ND5', 'CRO_ND30', 'CTX_ND30', 'CXM_ND30', 'ERY_ND15', 'ETP_ND10', 'FEP_ND30', 'FLC_ND', 'FOX_ND30', 'GEN_ND10', 'LEX_ND30', 'LIN_ND4', 'LNZ_ND30', 'LVX_ND5', 'MEM_ND10', 'MNO_ND30', 'OXA_ND1', 'PEN_ND10', 'PNV_ND10', 'RIF_ND5', 'SXT_ND1_2', 'TCY_ND30', 'TGC_ND15', 'TZP_ND100', 'VAN_ND30']\n",
      "\n",
      "Dataset shape: (32688, 45)\n",
      "Sample of key columns:\n",
      "  PATIENT_ID: 32688/32688 values\n",
      "  SPEC_DATE: 32688/32688 values\n",
      "  WHONET_ORG_CODE: NOT FOUND\n",
      "\n",
      "Country-related columns: ['COUNTRY']\n",
      "Institution-related columns: ['INSTITUTION']\n"
     ]
    }
   ],
   "source": [
    "# Quick inspection of cleaned dataset columns\n",
    "print(\"Cleaned dataset columns:\")\n",
    "print(df_cleaned.columns.tolist())\n",
    "print(f\"\\nDataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Sample of key columns:\")\n",
    "key_cols = ['PATIENT_ID', 'SPEC_DATE', 'WHONET_ORG_CODE']\n",
    "for col in key_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        print(f\"  {col}: {df_cleaned[col].notna().sum()}/{len(df_cleaned)} values\")\n",
    "    else:\n",
    "        print(f\"  {col}: NOT FOUND\")\n",
    "\n",
    "# Check for country and institution columns\n",
    "country_cols = [col for col in df_cleaned.columns if 'country' in col.lower() or 'pays' in col.lower()]\n",
    "institution_cols = [col for col in df_cleaned.columns if 'institut' in col.lower()]\n",
    "print(f\"\\nCountry-related columns: {country_cols}\")\n",
    "print(f\"Institution-related columns: {institution_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bffd8861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š GENERATING COMPREHENSIVE DATA QUALITY REPORT\n",
      "======================================================================\n",
      "ðŸ“… Report Generation Date: 2025-06-11\n",
      "â° Report Generation Time: 15:38:17\n",
      "\n",
      "1. Dataset Overview Compilation\n",
      "----------------------------------------\n",
      "âœ… Dataset overview compiled: 32,688 records, 30,081 patients\n",
      "\n",
      "2. Data Cleaning Summary Compilation\n",
      "----------------------------------------\n",
      "âœ… Data cleaning summary compiled: 3,485 records removed (9.63% reduction)\n",
      "\n",
      "3. WHO GLASS Compliance Assessment\n",
      "----------------------------------------\n",
      "âœ… WHO GLASS compliance assessed: 8/8 essential fields present\n",
      "\n",
      "4. Quality Metrics Calculation\n",
      "----------------------------------------\n",
      "âœ… Quality metrics calculated: 34 AST columns, 3.3% avg completeness\n",
      "\n",
      "5. Organism and Antimicrobial Analysis\n",
      "----------------------------------------\n",
      "âœ… Organism analysis completed: 76 unique organisms identified\n",
      "\n",
      "6. Compiling Final Report\n",
      "----------------------------------------\n",
      "âœ… Comprehensive quality report compiled successfully\n",
      "\n",
      "7. Report Export\n",
      "----------------------------------------\n",
      "âœ… JSON report exported: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\quality_reports\\comprehensive_quality_report_2025-06-11.json\n",
      "âœ… CSV summary exported: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\quality_reports\\quality_report_summary_2025-06-11.csv\n",
      "âœ… Detailed text report exported: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\quality_reports\\quality_report_detailed_2025-06-11.txt\n",
      "\n",
      "ðŸ“ All reports exported to: c:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\quality_reports\n",
      "ðŸ“Š Report files generated:\n",
      "   â€¢ comprehensive_quality_report_2025-06-11.json (Comprehensive JSON)\n",
      "   â€¢ quality_report_summary_2025-06-11.csv (Summary CSV)\n",
      "   â€¢ quality_report_detailed_2025-06-11.txt (Detailed Text)\n",
      "\n",
      "======================================================================\n",
      "âœ… COMPREHENSIVE DATA QUALITY REPORT GENERATION COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Comprehensive Data Quality Report Generation\n",
    "# \n",
    "# This section generates a comprehensive data quality report that summarizes \n",
    "# all data cleaning operations, quality metrics, and WHO GLASS compliance \n",
    "# assessments. The report will be exported in multiple formats for \n",
    "# documentation and audit purposes.\n",
    "#\n",
    "# Report Components:\n",
    "# 1. Dataset Overview: Basic statistics and data characteristics\n",
    "# 2. Data Cleaning Summary: Step-by-step processing results\n",
    "# 3. WHO GLASS Compliance: Essential fields and quality thresholds\n",
    "# 4. Quality Metrics: Completeness, consistency, and validity measures\n",
    "# 5. Export Documentation: Timestamped audit trail\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š GENERATING COMPREHENSIVE DATA QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare report timestamp\n",
    "report_timestamp = datetime.now()\n",
    "report_date = report_timestamp.strftime(\"%Y-%m-%d\")\n",
    "report_time = report_timestamp.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(f\"ðŸ“… Report Generation Date: {report_date}\")\n",
    "print(f\"â° Report Generation Time: {report_time}\")\n",
    "\n",
    "print(\"\\n1. Dataset Overview Compilation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate comprehensive dataset statistics\n",
    "original_records = len(df_raw)\n",
    "final_records = len(df_cleaned)\n",
    "total_reduction = original_records - final_records\n",
    "reduction_rate = (total_reduction / original_records) * 100\n",
    "\n",
    "dataset_overview = {\n",
    "    \"total_records\": final_records,\n",
    "    \"total_patients\": df_cleaned['PATIENT_ID'].nunique(),\n",
    "    \"date_range\": {\n",
    "        \"start\": df_cleaned['SPEC_DATE'].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"end\": df_cleaned['SPEC_DATE'].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"span_days\": (df_cleaned['SPEC_DATE'].max() - df_cleaned['SPEC_DATE'].min()).days\n",
    "    },\n",
    "    \"countries\": df_cleaned['COUNTRY'].unique().tolist(),\n",
    "    \"institutions\": df_cleaned['INSTITUTION'].nunique()\n",
    "}\n",
    "\n",
    "print(f\"âœ… Dataset overview compiled: {final_records:,} records, {dataset_overview['total_patients']:,} patients\")\n",
    "\n",
    "print(\"\\n2. Data Cleaning Summary Compilation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compile data cleaning steps summary\n",
    "data_cleaning_summary = {\n",
    "    \"initial_raw_records\": original_records,\n",
    "    \"final_clean_records\": final_records,\n",
    "    \"total_records_removed\": total_reduction,\n",
    "    \"total_reduction_rate\": f\"{reduction_rate:.2f}%\",\n",
    "    \"cleaning_steps\": {\n",
    "        \"step_1_column_mapping\": {\n",
    "            \"records_before\": original_records,\n",
    "            \"records_after\": original_records,  # Column mapping doesn't remove records\n",
    "            \"records_removed\": 0,\n",
    "            \"reduction_rate\": \"0.00%\",\n",
    "            \"description\": \"Column mapping and standardization\"\n",
    "        },\n",
    "        \"step_2_deduplication\": {\n",
    "            \"records_before\": original_records,\n",
    "            \"records_after\": final_records,\n",
    "            \"records_removed\": total_reduction,\n",
    "            \"reduction_rate\": f\"{reduction_rate:.2f}%\",\n",
    "            \"description\": \"First isolate per patient filtering\"\n",
    "        }\n",
    "    },\n",
    "    \"data_quality_improvements\": {\n",
    "        \"duplicate_records_removed\": total_reduction,\n",
    "        \"patient_organism_pairs_deduplicated\": True,\n",
    "        \"column_names_standardized\": len([k for k in COLUMN_MAPPING.keys() if k in df_raw.columns]),\n",
    "        \"who_glass_compliance_applied\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… Data cleaning summary compiled: {total_reduction:,} records removed ({reduction_rate:.2f}% reduction)\")\n",
    "\n",
    "print(\"\\n3. WHO GLASS Compliance Assessment\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Assess WHO GLASS compliance\n",
    "essential_field_completeness = {}\n",
    "for field in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED:\n",
    "    if field in df_cleaned.columns:\n",
    "        completeness = (df_cleaned[field].notna().sum() / len(df_cleaned)) * 100\n",
    "        essential_field_completeness[field] = f\"{completeness:.1f}%\"\n",
    "    else:\n",
    "        essential_field_completeness[field] = \"0.0% (Missing)\"\n",
    "\n",
    "# Calculate overall compliance metrics\n",
    "compliance_scores = {\n",
    "    \"essential_fields_present\": len([f for f in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED if f in df_cleaned.columns]),\n",
    "    \"total_essential_fields\": len(GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED),\n",
    "    \"minimum_completeness_met\": all(\n",
    "        (df_cleaned[f].notna().sum() / len(df_cleaned)) * 100 >= GLASS_QUALITY_THRESHOLDS['minimum_completeness']\n",
    "        for f in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED if f in df_cleaned.columns\n",
    "    ),\n",
    "    \"temporal_coverage_months\": (df_cleaned['SPEC_DATE'].max() - df_cleaned['SPEC_DATE'].min()).days / 30.44,\n",
    "    \"minimum_isolates_met\": len(df_cleaned) >= GLASS_QUALITY_THRESHOLDS['minimum_isolates']\n",
    "}\n",
    "\n",
    "who_glass_compliance = {\n",
    "    \"essential_fields_compliance\": f\"{compliance_scores['essential_fields_present']}/{compliance_scores['total_essential_fields']} fields present\",\n",
    "    \"data_completeness_compliance\": \"PASS\" if compliance_scores['minimum_completeness_met'] else \"REVIEW REQUIRED\",\n",
    "    \"temporal_coverage_compliance\": f\"{compliance_scores['temporal_coverage_months']:.1f} months\",\n",
    "    \"sample_size_compliance\": \"PASS\" if compliance_scores['minimum_isolates_met'] else \"INSUFFICIENT\",\n",
    "    \"first_isolate_rule_applied\": True,\n",
    "    \"deduplication_performed\": True\n",
    "}\n",
    "\n",
    "print(f\"âœ… WHO GLASS compliance assessed: {compliance_scores['essential_fields_present']}/{compliance_scores['total_essential_fields']} essential fields present\")\n",
    "\n",
    "print(\"\\n4. Quality Metrics Calculation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate comprehensive quality metrics\n",
    "ast_columns = [col for col in df_cleaned.columns if any(ast in col for ast in ['_ND', 'AST_'])]\n",
    "ast_completeness = []\n",
    "\n",
    "for col in ast_columns[:10]:  # Sample first 10 AST columns\n",
    "    completeness = (df_cleaned[col].notna().sum() / len(df_cleaned)) * 100\n",
    "    ast_completeness.append(completeness)\n",
    "\n",
    "avg_ast_completeness = np.mean(ast_completeness) if ast_completeness else 0\n",
    "\n",
    "quality_metrics = {\n",
    "    \"essential_field_completeness\": essential_field_completeness,\n",
    "    \"ast_columns_available\": len(ast_columns),\n",
    "    \"average_ast_completeness\": f\"{avg_ast_completeness:.1f}%\",\n",
    "    \"data_consistency_checks\": {\n",
    "        \"patient_id_format_consistent\": df_cleaned['PATIENT_ID'].dtype == 'object',\n",
    "        \"date_format_standardized\": df_cleaned['SPEC_DATE'].dtype == 'datetime64[ns]',\n",
    "        \"organism_codes_valid\": df_cleaned['ORGANISM_CODE'].notna().sum() > 0\n",
    "    },\n",
    "    \"missing_data_summary\": {\n",
    "        field: f\"{(df_cleaned[field].isna().sum() / len(df_cleaned)) * 100:.1f}%\"\n",
    "        for field in GLASS_ESSENTIAL_FIELDS_MAPPED_UPDATED if field in df_cleaned.columns\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… Quality metrics calculated: {len(ast_columns)} AST columns, {avg_ast_completeness:.1f}% avg completeness\")\n",
    "\n",
    "print(\"\\n5. Organism and Antimicrobial Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze organism distribution\n",
    "organism_counts = df_cleaned['ORGANISM_CODE'].value_counts()\n",
    "top_organisms = organism_counts.head(10).to_dict()\n",
    "\n",
    "# Department distribution\n",
    "department_distribution = df_cleaned['DEPARTMENT'].value_counts().to_dict()\n",
    "\n",
    "# Geographic distribution\n",
    "country_distribution = df_cleaned['COUNTRY'].value_counts().to_dict()\n",
    "institution_distribution = df_cleaned['INSTITUTION'].value_counts().to_dict()\n",
    "\n",
    "organism_analysis = {\n",
    "    \"total_unique_organisms\": len(organism_counts),\n",
    "    \"top_10_organisms\": top_organisms,\n",
    "    \"department_distribution\": department_distribution,\n",
    "    \"geographic_distribution\": {\n",
    "        \"countries\": country_distribution,\n",
    "        \"institutions\": len(institution_distribution),\n",
    "        \"institution_coverage\": f\"{len(institution_distribution)} facilities\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… Organism analysis completed: {len(organism_counts)} unique organisms identified\")\n",
    "\n",
    "print(\"\\n6. Compiling Final Report\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compile comprehensive quality report\n",
    "comprehensive_quality_report = {\n",
    "    \"report_metadata\": {\n",
    "        \"generation_date\": report_date,\n",
    "        \"generation_time\": report_time,\n",
    "        \"report_version\": \"1.0\",\n",
    "        \"data_source\": \"AMR_DATA_FINAL.csv\",\n",
    "        \"processing_pipeline\": \"WHO GLASS Compliant Data Cleaning\",\n",
    "        \"analyst\": \"Automated Data Processing System\"\n",
    "    },\n",
    "    \"dataset_overview\": dataset_overview,\n",
    "    \"data_cleaning_summary\": data_cleaning_summary,\n",
    "    \"who_glass_compliance\": who_glass_compliance,\n",
    "    \"quality_metrics\": quality_metrics,\n",
    "    \"organism_analysis\": organism_analysis,\n",
    "    \"recommendations\": {\n",
    "        \"data_quality_grade\": \"HIGH\" if avg_ast_completeness > 80 else \"MODERATE\" if avg_ast_completeness > 60 else \"REQUIRES_IMPROVEMENT\",\n",
    "        \"who_glass_compliance_status\": \"COMPLIANT\" if compliance_scores['minimum_completeness_met'] else \"REVIEW_REQUIRED\",\n",
    "        \"next_steps\": [\n",
    "            \"Regular quality monitoring\",\n",
    "            \"Periodic WHO GLASS compliance review\",\n",
    "            \"Continuous data validation\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Comprehensive quality report compiled successfully\")\n",
    "\n",
    "print(\"\\n7. Report Export\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create export directory\n",
    "export_dir = PROCESSED_DATA_PATH / \"quality_reports\"\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export as JSON\n",
    "json_filename = f\"comprehensive_quality_report_{report_date}.json\"\n",
    "json_path = export_dir / json_filename\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(comprehensive_quality_report, f, indent=4, ensure_ascii=False, default=str)\n",
    "    print(f\"âœ… JSON report exported: {json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error exporting JSON report: {e}\")\n",
    "\n",
    "# Export summary as CSV\n",
    "try:\n",
    "    summary_data = []\n",
    "    \n",
    "    # Basic metrics\n",
    "    summary_data.append([\"Metric\", \"Value\", \"Category\"])\n",
    "    summary_data.append([\"Total Records\", final_records, \"Dataset Overview\"])\n",
    "    summary_data.append([\"Total Patients\", dataset_overview['total_patients'], \"Dataset Overview\"])\n",
    "    summary_data.append([\"Date Range (Days)\", dataset_overview['date_range']['span_days'], \"Dataset Overview\"])\n",
    "    summary_data.append([\"Records Removed\", total_reduction, \"Data Cleaning\"])\n",
    "    summary_data.append([\"Reduction Rate (%)\", f\"{reduction_rate:.2f}\", \"Data Cleaning\"])\n",
    "    summary_data.append([\"WHO GLASS Compliance\", who_glass_compliance['data_completeness_compliance'], \"Quality\"])\n",
    "    summary_data.append([\"AST Completeness (%)\", f\"{avg_ast_completeness:.1f}\", \"Quality\"])\n",
    "    \n",
    "    # Essential field completeness\n",
    "    for field, completeness in essential_field_completeness.items():\n",
    "        summary_data.append([f\"{field} Completeness\", completeness, \"Essential Fields\"])\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data[1:], columns=summary_data[0])\n",
    "    csv_filename = f\"quality_report_summary_{report_date}.csv\"\n",
    "    csv_path = export_dir / csv_filename\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… CSV summary exported: {csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error exporting CSV summary: {e}\")\n",
    "\n",
    "# Export detailed text report\n",
    "try:\n",
    "    txt_filename = f\"quality_report_detailed_{report_date}.txt\"\n",
    "    txt_path = export_dir / txt_filename\n",
    "    \n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"WHO GLASS AMR DATA QUALITY REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Report Generated: {report_date} {report_time}\\n\")\n",
    "        f.write(f\"Data Source: AMR_DATA_FINAL.csv\\n\")\n",
    "        f.write(f\"Processing Pipeline: WHO GLASS Compliant Data Cleaning\\n\\n\")\n",
    "        \n",
    "        f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(f\"â€¢ Total Records Processed: {final_records:,}\\n\")\n",
    "        f.write(f\"â€¢ Unique Patients: {dataset_overview['total_patients']:,}\\n\")\n",
    "        f.write(f\"â€¢ Data Reduction Rate: {reduction_rate:.2f}%\\n\")\n",
    "        f.write(f\"â€¢ WHO GLASS Compliance: {who_glass_compliance['data_completeness_compliance']}\\n\")\n",
    "        f.write(f\"â€¢ Average AST Completeness: {avg_ast_completeness:.1f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"ESSENTIAL FIELDS COMPLETENESS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for field, completeness in essential_field_completeness.items():\n",
    "            f.write(f\"â€¢ {field}: {completeness}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTOP 5 ORGANISMS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for i, (organism, count) in enumerate(list(top_organisms.items())[:5], 1):\n",
    "            f.write(f\"{i}. {organism}: {count:,} isolates\\n\")\n",
    "        \n",
    "        f.write(f\"\\nDATA QUALITY RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for rec in comprehensive_quality_report['recommendations']['next_steps']:\n",
    "            f.write(f\"â€¢ {rec}\\n\")\n",
    "    \n",
    "    print(f\"âœ… Detailed text report exported: {txt_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error exporting text report: {e}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All reports exported to: {export_dir}\")\n",
    "print(f\"ðŸ“Š Report files generated:\")\n",
    "print(f\"   â€¢ {json_filename} (Comprehensive JSON)\")\n",
    "print(f\"   â€¢ {csv_filename} (Summary CSV)\")\n",
    "print(f\"   â€¢ {txt_filename} (Detailed Text)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… COMPREHENSIVE DATA QUALITY REPORT GENERATION COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87414b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“¤ EXPORTING CLEANED AND DEDUPLICATED DATASET\n",
      "======================================================================\n",
      "ðŸ“Š Exporting cleaned dataset to: C:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\deduplicated\\df_cleaned_2025-06-11.csv\n",
      "âœ… Export completed successfully!\n",
      "ðŸ“ File location: C:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\deduplicated\\df_cleaned_2025-06-11.csv\n",
      "ðŸ“Š Records exported: 32,688\n",
      "ðŸ“¦ File size: 3.14 MB\n",
      "ðŸ“‹ Columns exported: 45\n",
      "ðŸ“ Export metadata saved: C:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\deduplicated\\export_metadata_2025-06-11.json\n",
      "\n",
      "======================================================================\n",
      "âœ… DATASET EXPORT COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned and deduplicated dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“¤ EXPORTING CLEANED AND DEDUPLICATED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create the export directory\n",
    "export_path = Path(r\"C:\\NATIONAL AMR DATA ANALYSIS FILES\\data\\processed\\deduplicated\")\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate timestamped filename\n",
    "export_filename = f\"df_cleaned_{report_date}.csv\"\n",
    "export_filepath = export_path / export_filename\n",
    "\n",
    "try:\n",
    "    # Export the cleaned dataframe\n",
    "    print(f\"ðŸ“Š Exporting cleaned dataset to: {export_filepath}\")\n",
    "    df_cleaned.to_csv(export_filepath, index=False)\n",
    "    \n",
    "    # Verify the export\n",
    "    exported_size = export_filepath.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    \n",
    "    print(f\"âœ… Export completed successfully!\")\n",
    "    print(f\"ðŸ“ File location: {export_filepath}\")\n",
    "    print(f\"ðŸ“Š Records exported: {len(df_cleaned):,}\")\n",
    "    print(f\"ðŸ“¦ File size: {exported_size:.2f} MB\")\n",
    "    print(f\"ðŸ“‹ Columns exported: {len(df_cleaned.columns)}\")\n",
    "    \n",
    "    # Create export metadata\n",
    "    export_metadata = {\n",
    "        \"export_timestamp\": datetime.now().isoformat(),\n",
    "        \"original_file\": \"AMR_DATA_FINAL.csv\",\n",
    "        \"export_file\": export_filename,\n",
    "        \"records_exported\": len(df_cleaned),\n",
    "        \"columns_exported\": len(df_cleaned.columns),\n",
    "        \"file_size_mb\": round(exported_size, 2),\n",
    "        \"data_processing_summary\": {\n",
    "            \"column_mapping_applied\": True,\n",
    "            \"deduplication_applied\": True,\n",
    "            \"who_glass_compliance\": True,\n",
    "            \"records_removed\": total_reduction,\n",
    "            \"reduction_rate_percent\": round(reduction_rate, 2)\n",
    "        },\n",
    "        \"column_list\": df_cleaned.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    # Save metadata file\n",
    "    metadata_filename = f\"export_metadata_{report_date}.json\"\n",
    "    metadata_filepath = export_path / metadata_filename\n",
    "    \n",
    "    with open(metadata_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_metadata, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"ðŸ“ Export metadata saved: {metadata_filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during export: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATASET EXPORT COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
